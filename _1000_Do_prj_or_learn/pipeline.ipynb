{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020de32a",
   "metadata": {},
   "source": [
    "## Import necessary or common package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /home/user/pandas-polar-dask/_1000_Do_prj_or_learn/_1_allLibs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full._1_allLibs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c2d8e",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bd672",
   "metadata": {},
   "source": [
    "- Logger\n",
    "- Benchmark\n",
    "- Profile\n",
    "- Time\n",
    "- Print chart\n",
    "- Export report\n",
    "- Read file: csv, json, parquet, csv, zip, database by chunk about 1 milllion rows or 10GB\n",
    "- Concat data\n",
    "- Drop columns\n",
    "- Write file: the same as read file\n",
    "- Crawl data: html table with pagination,...\n",
    "- Call API with pagination\n",
    "- Alert\n",
    "- Extract from pdf, doc,..\n",
    "\n",
    "- Normalize column name\n",
    "\n",
    "- Read config\n",
    "- Get local time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33e611",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169ceae",
   "metadata": {},
   "source": [
    "<h1>Build data pipeline from scratch</h1>\n",
    "<h3>__author__: Pham ANh Nhat</h3>\n",
    "<h3>__email__: truonghoc19102001@gmail.com</h3>\n",
    "<h3>__createdOn__: 2025-09-08</h3>\n",
    "<h3>__modifiedOn__: 2025-09-08</h3>\n",
    "<h3>__version__: 1.0</h3>\n",
    "<h3>Source, note and other storage:</h3>\n",
    "<ul>\n",
    "<li>Note pandas, polars, dask,..: https://docs.google.com/document/d/1FW92fjuOvik8c3ojF6cY0pqJy6NCYmt5XT6XavWeVAI/edit?tab=t.emds46nzbmbz</li>\n",
    "<li> Note data relates (DS, DA, data source,...): https://docs.google.com/document/d/1K3t01SniGOJSfFsJCh7Ma9YI28XAkni8S7S-JQkTC_8/edit?tab=t.h4am2tdi4jqw</li>\n",
    "<li>Tool using: https://docs.google.com/document/d/1Omu4JL-y52KUacSKk70sUp3Cb6nUoC4MiyO6Dmc-_pM/edit?tab=t.oo3b2e7vxdwn</li>\n",
    "</ul>\n",
    "<h3>Note: I think/ willing to build, will (*) in start</h3>\n",
    "# I. What is data pipeline?\n",
    "workfow to transfer and porcess data from source to destination\n",
    "\n",
    "# 2. How many pharses?\n",
    "## 1. Data collection: collect data from many source:\n",
    "### a. Source\n",
    "- API\n",
    "- Web, by crawl data: html (table, comment, ...), xml,...\n",
    "- Csv file\n",
    "- Zip file\n",
    "- Json file\n",
    "- Parquet file\n",
    "- Database: sql (postgresql,...), nosql(mongodb,..)\n",
    "- Other\n",
    "\n",
    "### b. What need do in this pharse:\n",
    "- Plan stragedy collect data, type data need collect (csv, json, html,...), type data storafe, where storage, time, storage size (local and cloud), topics to do (weather, layoffs, unemploment,...), tool, technique to do (db: nosql(mongodb) or sql(sql lite, postgresql) or other (duckdb,..), lang(python, js, ts,..), lib(pandas/polar/dask/...), test(jest/pytest/unitest), manage(pip/uv/poetry), log(loguru.ice,..), benchmark, profile(cProfile/memory-profiler, tuna/austin/...),...), open source or cloud, estimate time and cost (fee,..)\n",
    "- Build pipeline architecture and data processing architecture: batch, streaming, kappa, lambda,..; ETL, ELT, ETLT,..\n",
    "- Draw/visualize pipeline, step,...\n",
    "- Define common method, format,.. like date format, nameing variable, file name, folder, code style,..\n",
    "- (*) Log process to terminal to see now, to log file for history and inspect, serialize(json) for dashboard realtime (optional)\n",
    "- (*) Add timestamp, version to watch out\n",
    "- (*) (Optional) Dasboard or chart to present how mnay data collected, time effort, memory usage,..\n",
    "- (*) Build prcoess, design work flow, step detail\n",
    "- (*) Build function to crawl data, read data and write data, for large data (>= 1 million records or >= 1GB), detect encode, determiner (if need), recovery, benchmark performance\n",
    "- (*) Note work did\n",
    "- (*) Confilg file, url, tag file to recognize data crawled or not\n",
    "- (*) Check file exists or not, if exist, ignore, else, check link is active or not, if active, log and start crawl and store data to file as sql/csv (in my sitiuation, 2025-09-05, my computer is broken, using company computer to work, so save in csv for easy migration and back up), else log error and continue other collection actions, else end program if not exist any file/link need executes\n",
    "\n",
    "### c. Requirement\n",
    "1. Functional requirement\n",
    "- Suitable for may type of data\n",
    "- Easy to modify if have any change\n",
    "- High performance: low time and memery cost, can handle large data and data tang dot bien, dynamic data/collection, can recover quickly after broken\n",
    "- Alert clear\n",
    "- Log clear, easy to find issue\n",
    "- Benchmark clear, visualize\n",
    "\n",
    "2. Non-functional requirement\n",
    "- Clear pipeline with clear step, easy to process\n",
    "- Easy to use\n",
    "- Well structured (folder structure, file storage, format,..)\n",
    "- Recovery well\n",
    "- Easy to maintain\n",
    "- Essy to widen\n",
    "- Resusable\n",
    "- Clean code\n",
    "- Note, doc clear and easy to read, find. Short but enough\n",
    "- Testing ki cang\n",
    "\n",
    "### d. Reference\n",
    "- [1] Title, Author, yyyy-mm-dd, link: a/b/c\n",
    "\n",
    "## 2. Data ingestion: load data to staging storage\n",
    "### a. What need do?\n",
    "- Build suitable method to load data for each type of data: batch data/ streaming data, csv/json/sql/..\n",
    "- Convert to only 1 format type: sql\n",
    "- Save data to next staging storage\n",
    "- Logging\n",
    "- Testing\n",
    "- Change data captre (CDC): Add timestamp/version for data, using for update/delete data in data storage pharse\n",
    "- Save data to union format (sql or csv, in my case, may be csv fo easy migration, and data is not so much, if having constant storage or computer, maybe save by sql)\n",
    "\n",
    "### b. Requirement\n",
    "1. Functional\n",
    "\n",
    "2. Non-functional\n",
    "\n",
    "## 3. Data computation: most important, data quality is better than model\n",
    "### a. What need do?\n",
    "* Pre processing:\n",
    "1.  EDA: Explode data analyst\n",
    "- df.shape\n",
    "- df.info\n",
    "- df.describe\n",
    "- df.head/tail\n",
    "- df.notna()\n",
    "- df.isna().sum(): number of na value per column\n",
    "- df.agg(['count', 'size', 'nunique']): nunique: number of unique value per column\n",
    "- df.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True): count value per column\n",
    "- df.count(): count not na value per column\n",
    "- df.duplicated(): find duplicate value\n",
    "- df.astype(type): cast type\n",
    "- df.select_dtypes(include=\"number\").astype(\"Int64\")\n",
    "- df.infer_objects()\n",
    "- pd.read_excel(file_path, dtype=\"str\")\n",
    "- df.infer_objects()\n",
    "- outliner\n",
    "- format\n",
    "\n",
    "2.  Visualize data to explode data\n",
    "* Processing\n",
    "1. Data cleaning\n",
    "- Remove duplicate\n",
    "- Fill empty/missing value\n",
    "- Remove invalid value, outliner\n",
    "- Filter data\n",
    "- Type casting\n",
    "\n",
    "#### Some pandas function can use in this pharse:\n",
    "- df.rename(columns = [] / columns={old_col: new_col,..})\n",
    "- df.copy(deep=True)\n",
    "- fillna by const value, mean, std, threshold,..\n",
    "- bfill/ffill\n",
    "- dropna\n",
    "- drop_duplicates\n",
    "- remove outliner: using Z-score/IQR/…\n",
    "- castype\n",
    "- change format datetime,...\n",
    "- re calculation/ refill data\n",
    "- remove unnecessary cols\n",
    "<!-- ?###aaaaaa -->\n",
    "2. Data transformation\n",
    "- Increment\n",
    "- Enrichment\n",
    "- Aggregation\n",
    "- Intergration\n",
    "- Reduction\n",
    "- Normalization\n",
    "- Standardization\n",
    "- Deliver column: create new column by using current columns\n",
    "- Encoding categorical variables\n",
    "- Feature engineering and extraction\n",
    "- Feature selection\n",
    "- Principal component analysis (PCA)\n",
    "- Sampling methods\n",
    "- Data deduplication\n",
    "- Schema matching\n",
    "\n",
    "3. Data increment\n",
    "\n",
    "- Which function in pandas suitable for each step?\n",
    "- Define processing methods need, lib/package need\n",
    "- OLAP\n",
    "- OTPC??\n",
    "- Data lake, house, ...\n",
    "- Medalizon architecture,...\n",
    "\n",
    "### b. Requirement\n",
    "1. Functional\n",
    "- Build processing pipeline\n",
    "- Find pandas function suitable for each step -> build pipline\n",
    "\n",
    "2. Non-functional\n",
    "- The same previous pharses\n",
    "- Toi uu\n",
    "- Phat hien va xu li bat thuong\n",
    "- Phuc hoi\n",
    "\n",
    "## 4. Data storage\n",
    "### a. What need do?\n",
    "- Load data to storage (sql)\n",
    "- Update old data with new info\n",
    "- Delete old data that unnecessary\n",
    "- Classification: dim data (history info, definition info) and fact (id relation, data need for next pharses)\n",
    "- Modelling\n",
    "- Check quality\n",
    "- Log\n",
    "- Refactor - to chuc lai data\n",
    "- Toi uu\n",
    "\n",
    "### b. Requirement\n",
    "1. Functional\n",
    "\n",
    "2. Non-functional\n",
    "- The same as previous pharse\n",
    "- Always ready data for using\n",
    "- No error\n",
    "- Good sap xep\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "- Unique storage: sql/csv/nosql, unique format, backup and recovery\n",
    "- Maintain\n",
    "- Log\n",
    "- Checkout bat thuong\n",
    "- Process bat thuong\n",
    "Ensure data quality, suitable and read for next actions like: report, analyst, AI/ML,..\n",
    "Future: using more tool like dbt, apache airflow, cloud storage like azure, awb,.., cloud process like snowflake, databrick,..\n",
    "\n",
    "First, pay attention to data cleaning and visualizing, then data collection, thanh thao pandas, sql, excel, matplotlib, altair, tableau,..\n",
    "\n",
    "Other topics relates:\n",
    "Data mining\n",
    "OLTP vs OLAP\n",
    "\n",
    "Source:\n",
    "[1] Bước ETL trong Xử lý dữ liệu (Data Science), AI VIETNAM, 2025-08-29, https://www.youtube.com/watch?v=XvErOzKgyFE&list=PLY8SjtSzFw7R9B1_L_GDfl1rNEPcCSMfM&index=68&t=8980s\n",
    "[2] Data Zero to Hero - Khám phá cấu trúc Kho Dữ Liệu (DWH), Bành Cẩm Vỉnh, 2025-08-26, https://www.youtube.com/watch?v=mM2vWquaupk&list=PLY8SjtSzFw7R9B1_L_GDfl1rNEPcCSMfM&index=75&t=20s\n",
    "[3] What is Data Pipeline? | Why Is It So Popular?, ByByteGo, 2024, https://www.youtube.com/watch?v=kGT4PcTEPP8&list=PLY8SjtSzFw7R9B1_L_GDfl1rNEPcCSMfM&index=59\n",
    "[4] What are some common data pipeline design patterns? What is a DAG ? | ETL vs ELT vs CDC (2022), IT k Funde, 2022, https://www.youtube.com/watch?v=v67JHa4MrnQ&list=PLY8SjtSzFw7R9B1_L_GDfl1rNEPcCSMfM&index=25\n",
    "[5] What is Data Pipeline | How to design Data Pipeline ? - ETL vs Data pipeline (2025), IT k Funde, 2020, updated 2025, https://www.youtube.com/watch?v=VtzvF17ysbc&list=PLY8SjtSzFw7R9B1_L_GDfl1rNEPcCSMfM&index=33&t=93s\n",
    "\n",
    "My storage collection:\n",
    "[1] Data analyst, youtube playlist, https://www.youtube.com/playlist?list=PLY8SjtSzFw7R9B1_L_GDfl1rNEPcCSMfM\n",
    "[2] Data science, youtube playlist: https://www.youtube.com/playlist?list=PLY8SjtSzFw7RaC3yrWtcqxmWw-LBCG6jl\n",
    "[3] Python, youtube playlist: https://www.youtube.com/playlist?list=PLY8SjtSzFw7Touomd9MQxVcCyLWA_78S3\n",
    "\n",
    "\n",
    "Book learn data:\n",
    "[1] Python for Data Analysis - DATA WRANGLING WITH PANDAS, NUMPY, AND IPYTHON\n",
    ", Wes McKinney, 2nd edition, 2017-10: https://drive.google.com/drive/search?q=Python%20for%20Data%20Analysis\n",
    "[2] Python for Data Analysis, 3nd version, ebook: https://wesmckinney.com/book *\n",
    "(1, 2 is the same)\n",
    "[3] Practical Statistics for Data Scientists 50+ Essential Concepts Using R and Python - Peter Bruce, Andrew Bruce, and Peter Gedeck:\n",
    "https://drive.google.com/file/d/1NLsRZaqPV-Kn_agAk9kpoeyjjvCph1N2/view?usp=sharing\n",
    "[4] Fundamentals of Data Visualization - Claus O. Wilke: \n",
    "https://drive.google.com/file/d/1MVIDkFy5pcyRIiznUuuX_q2qsUk3OYo2/view?usp=sharing\n",
    "[5] Pandas 1.x Cookbook Practical recipes for scientific computing, time series analysis, and exploratory data analysis using Python, Matt Harrison &Theodore Petrou: https://soclibrary.futa.edu.ng/books/Pandas%201.x%20Cookbook%20Practical%20recipes%20for%20scientific%20computing,%20time%20series%20analysis,%20and%20exploratory%20data%20analysis%20using%20Python%20by%20Matt%20Harrison,%20Theodore%20Petrou%20(z-lib.org).pdf *\n",
    "effective pandas\n",
    "[6] https://drive.google.com/drive/search?q=Python%20for%20Data%20Analysis *\n",
    "https://python-course.eu/books/bernd_klein_python_data_analysis_a4.pdf\n",
    "https://file.cz123.top/5textbook/CODING/Numerical_Python.pdf\n",
    "[7] The Data Engineering Cookbook: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "[8] Data Engineering - Mining, Information and Intelligence: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "\n",
    "\n",
    "Github repo learn data\n",
    "\n",
    "\n",
    "Github repo project can refer\n",
    "https://github.com/ddgope/Data-Pipelines-with-Airflow/tree/master\n",
    "https://github.com/mikeacosta/airflow-data-pipeline\n",
    "https://github.com/godatadriven/data-pipelines-with-airflow-2nd-ed?tab=readme-ov-file\n",
    "https://github.com/saurabh2mishra/airflow-notes?tab=readme-ov-file\n",
    "\n",
    "Project idea:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fa3d7",
   "metadata": {},
   "source": [
    "Source detail\n",
    "\n",
    "# 1. Data collection\n",
    "# 2. Data ingestion\n",
    "[1] Data pipeline quality: Influencing factors, root causes of data-related issues, and processing problem areas for developers: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ **\n",
    "[2] Data pipeline monitoring solution and data quality in manufacturing company: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "\n",
    "\n",
    "# 3. Data compution\n",
    "## 1. Data cleaning\n",
    "[1] Python for Data Analysis, 3rd edition: https://wesmckinney.com/book/data-cleaning\n",
    "[2] https://github.com/kanishkamisra/Data-Science-Books/blob/master/tidy-data.pdf\n",
    "[3] https://github.com/kanishkamisra/Data-Science-Books/blob/master/effective-pandas.pdf\n",
    "[4] Advanced Data Analytics Using Python, Chapter 2: ETL with Python (Structured Data), page 23: https://drive.google.com/drive/search?q=Python%20for%20Data%20Analysis\n",
    "[5] Matplotlib for python dev: https://drive.google.com/drive/search?q=Python%20for%20Data%20Analysis\n",
    "[6] Effective pandas: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "Other\n",
    "https://en.wikipedia.org/wiki/Extract,_transform,_load\n",
    "Basic data cleaning: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "Tidy data: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "The Data Engineer’s Guide to Apache spark: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "Python polars: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "\n",
    "# 4. Data storage\n",
    "[1] Data warehouse toolkit: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "[2] Building the Data Warehouse, 3rd edition: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "[3] Deciphering Data Architectures: Choosing Between a Modern Data Warehouse, Data Fabric, Data Lakehouse, and Data Mesh (slide): https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n",
    "[4] Building a modern Data Warehouse from scratch: https://drive.google.com/drive/folders/1JQ8RNJ04DpA3Wpd3VEFlgPizvuklmGsJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78049ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c42a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
