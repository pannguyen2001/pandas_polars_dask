{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To run other package from other file, we can : \n",
    "* Way 1: \n",
    "1. !pip install ipynb\n",
    "2. from ipynb.fs.full.<filename.ipynb> import * \n",
    "* Way 2: \n",
    "1. pip install nbformat nbclient\n",
    "2. %run <filename.ipynb> import * \n",
    "* Way 3: run directly: %run <file_name>.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package from other .ipynb file\n",
    "from ipynb.fs.full._1_allLibs import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import other .py file to .ipynb file\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.abspath(''))))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from util.logger.logger import logger\n",
    "from app import ic\n",
    "import cProfile\n",
    "import pstats\n",
    "ic('Hello')\n",
    "logger.info('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read large csv/excel file by using chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/user/pandas-polar-dask/data/time_series_covid19_recovered_global_narrow.csv'\n",
    "chunk_size =500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read csv by using context manager\n",
    "@logger.catch\n",
    "def readLargerFileCSV(file_path: str = '', chunk_size: int = 500) -> pd.DataFrame:\n",
    "    '''\n",
    "        Read large file csv by using chunk\n",
    "        Arguments: \n",
    "            - file_path (str): file path of csv file\n",
    "            - chunk_size (int): chunk size of csv file\n",
    "        Return:\n",
    "            - df: dataframe of csv file\n",
    "    '''\n",
    "    chunks = []\n",
    "    with pd.read_csv(file_path, chunksize = chunk_size) as reader:\n",
    "        for chunk in reader:\n",
    "            chunks.append(chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measure performance (time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit readLargerFileCSV(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('readLargerFileCSV(file_path)', 'csv_profile.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('csv_profile.prof')\n",
    "p.sort_stats('cumtime').print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('pd.read_csv(file_path)', 'read_csv.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('read_csv.prof')\n",
    "p.sort_stats('cumtime').print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "@logger.catch\n",
    "def readLargerFileCSV2(file_path: str = '', chunk_size: int = 500) -> pd.DataFrame:\n",
    "    '''\n",
    "    Efficiently read a large CSV file in chunks and return a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        chunk_size (int): Number of rows to read per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame from all chunks.\n",
    "    '''\n",
    "    # reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    # return pd.concat(tqdm(reader, desc=\"Reading CSV in chunks\"), ignore_index=True)\n",
    "    return (\n",
    "        pd.concat(\n",
    "            pd.read_csv(file_path, chunksize=chunk_size),\n",
    "             ignore_index=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit readLargerFileCSV2(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import Progress\n",
    "import pandas as pd\n",
    "\n",
    "@logger.catch\n",
    "def readLargerFileCSV3(file_path: str = '', chunk_size: int = 500) -> pd.DataFrame:\n",
    "    chunks = []\n",
    "    reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    \n",
    "    with Progress() as progress:\n",
    "        # Estimate total chunks (optional, only if you want progress %)\n",
    "        total_lines = sum(1 for _ in open(file_path)) - 1  # minus header\n",
    "        total_chunks = (total_lines // chunk_size) + 1\n",
    "        \n",
    "        task = progress.add_task(\"[cyan]Reading CSV...\", total=total_chunks)\n",
    "        \n",
    "        for chunk in reader:\n",
    "            chunks.append(chunk)\n",
    "            progress.update(task, advance=1)\n",
    "\n",
    "    return pd.concat(chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSVWithChunk(file_path: str = '', chunk_size: str = chunk_size):\n",
    "    reader = pd.read_csv(file_path, chunksize= chunk_size)\n",
    "    for chunk in  reader:\n",
    "        yield(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Usage\n",
    "def genChunk():\n",
    "    gen = readCSVWithChunk(file_path)\n",
    "    try:\n",
    "        i = 0\n",
    "        while True:\n",
    "            chunk = next(gen)\n",
    "            i+=1\n",
    "            print(f\"Chunk {i} has shape:\", chunk.shape)\n",
    "    except StopIteration:\n",
    "        print(\"âœ… All chunks have been processed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit genChunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(index=False, deep=True) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(index=False, deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def calc_chunksize(df, share=0.3):\n",
    "    \"\"\"Estimate optimal chunksize (in records) for writing large dfs with df.to_csv\"\"\"\n",
    "    print('df shape',df.shape)\n",
    "    # get approximate record size in bytes\n",
    "    row_size = df.memory_usage(index=True, deep=True).sum() / df.shape[0]\n",
    "    print(f'Avg row size: {row_size:2f} bytes ({row_size / 1024 / 1024:2f} MB)')\n",
    "    # get share of available memory size in bytes\n",
    "    avail_mem = psutil.virtual_memory().available * share\n",
    "    # share is percent of avaliable memory to use for df\n",
    "    return int(avail_mem / row_size)\n",
    "\n",
    "calc_chunksize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_size = df.memory_usage(index=True, deep=True).sum() / df.shape[0]\n",
    "row_size #bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memorySize = 500 *1024 *1024 # assume that is maximum memory for df\n",
    "availableChunkSize = memorySize/ row_size\n",
    "availableChunkSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "getsizeof(df)/ len(df) # avg size per row by bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import concurrent.futures \n",
    "def process_chunk(chunk):\n",
    "    # Perform some data processing here\n",
    "    print(f\"Processing {len(chunk)} records\")\n",
    "    return \"Chunk processed\"\n",
    "\n",
    "chunk_iter = pd.read_csv(file_path, chunksize=10000)\n",
    "\n",
    "# Use ThreadPoolExecutor to process chunks in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit each chunk to the executor to be processed in parallel\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunk_iter]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
